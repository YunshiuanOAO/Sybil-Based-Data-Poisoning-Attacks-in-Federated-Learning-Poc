"""
Sybil-based Virtual Data Poisoning Attack Module
===============================================

åŸºæ–¼è«–æ–‡: "Sybil-based Virtual Data Poisoning Attacks in Federated Learning"
å¯¦ç¾ç„¡ç›®æ¨™ (Untargeted) æ”»æ“Šè®Šé«”

æ ¸å¿ƒåŠŸèƒ½ï¼š
- è™›æ“¬æ•¸æ“šç”Ÿæˆ (åŸºæ–¼æ¢¯åº¦åŒ¹é…)
- ç›®æ¨™æ¨¡å‹ç²å– (Online Global æ–¹æ¡ˆ)
- Sybil ç¯€é»ç®¡ç†
- ç„¡ç›®æ¨™æŠ•æ¯’æ”»æ“Š

Author: Security Research Team
Date: 2024
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import copy
import json
import random
from datetime import datetime
from typing import List, Dict, Tuple, Any
from environment import FederatedLearningEnvironment

class SybilVirtualDataAttackOrchestrator:
    """
    åŸºæ–¼è«–æ–‡çš„ Sybil è™›æ“¬æ•¸æ“šæŠ•æ¯’æ”»æ“Šç·¨æ’å™¨
    å¯¦ç¾ç„¡ç›®æ¨™æ”»æ“Š (Untargeted Attack)
    """
    
    def __init__(self, environment: FederatedLearningEnvironment, num_sybil_per_malicious: int = 5):
        self.environment = environment
        self.attack_history = []
        self.current_round = 0
        self.attack_active = False
        self.attack_start_round = 0
        
        # è™›æ“¬æ•¸æ“šæ”»æ“Šåƒæ•¸
        self.num_sybil_per_malicious = num_sybil_per_malicious  # æ¯å€‹æƒ¡æ„å®¢æˆ¶ç«¯çš„ sybil ç¯€é»æ•¸
        self.perturbation_lr = 0.1  # æ“¾å‹•å­¸ç¿’ç‡
        self.max_perturbation_iters = 50  # æœ€å¤§æ“¾å‹•è¿­ä»£æ¬¡æ•¸
        self.virtual_datasets = {}  # å­˜å„²è™›æ“¬æŠ•æ¯’æ•¸æ“š
        self.target_model = None  # ç›®æ¨™æ¨¡å‹
        
    def acquire_target_model_online_global(self, global_model: nn.Module) -> nn.Module:
        """
        ç›®æ¨™æ¨¡å‹ç²å– - Online Global æ–¹æ¡ˆ (ä¿®å¾©ç‰ˆ)
        å‰µå»ºä¸€å€‹æ•…æ„æ€§èƒ½è¼ƒå·®çš„ç›®æ¨™æ¨¡å‹ä¾†æŒ‡å°æ”»æ“Šæ–¹å‘
        """
        if len(self.environment.sybil_clients) == 0:
            return copy.deepcopy(global_model)
            
        # å‰µå»ºä¸€å€‹æ•…æ„é™ç´šçš„ç›®æ¨™æ¨¡å‹
        target_model = copy.deepcopy(global_model)
        target_model.train()
        
        # ä½¿ç”¨é«˜å­¸ç¿’ç‡å’ŒéŒ¯èª¤æ¢¯åº¦æ–¹å‘ä¾†é™ç´šæ¨¡å‹
        optimizer = torch.optim.SGD(target_model.parameters(), lr=0.1)  # é«˜å­¸ç¿’ç‡
        criterion = nn.CrossEntropyLoss()
        
        # æ”¶é›†ä¸€äº›æ•¸æ“šä¾†é€²è¡Œåå‘è¨“ç·´
        collected_data = []
        collected_labels = []
        
        for client in self.environment.sybil_clients:
            for batch_idx, (data, target) in enumerate(client.data_loader):
                if batch_idx >= 3:  # é™åˆ¶æ‰¹æ¬¡
                    break
                collected_data.append(data)
                collected_labels.append(target)
        
        if collected_data:
            all_data = torch.cat(collected_data, dim=0)
            all_labels = torch.cat(collected_labels, dim=0)
            
            # åŸ·è¡Œ"åå‘è¨“ç·´"ä¾†é™ç´šæ¨¡å‹
            for step in range(10):  # å¤šæ­¥é™ç´š
                optimizer.zero_grad()
                output = target_model(all_data)
                
                # ç­–ç•¥1: ä½¿ç”¨åè½‰çš„æ¨™ç±¤ (ç„¡ç›®æ¨™æ”»æ“Šçš„æœ‰æ•ˆæ–¹å¼)
                flipped_labels = (all_labels + torch.randint(1, self.environment.num_classes, all_labels.shape)) % self.environment.num_classes
                loss = criterion(output, flipped_labels)
                
                # ç­–ç•¥2: æ·»åŠ å™ªè²ä¾†ç ´å£æ¬Šé‡
                loss.backward()
                
                # åœ¨æ›´æ–°å‰æ·»åŠ å™ªè²
                for param in target_model.parameters():
                    if param.grad is not None:
                        param.grad += torch.randn_like(param.grad) * 0.01
                
                optimizer.step()
                    
        return target_model
    
    def gradient_matching_perturbation(self, base_data: torch.Tensor, base_labels: torch.Tensor, 
                                     global_model: nn.Module, target_model: nn.Module) -> torch.Tensor:
        """
        åŸºæ–¼æ¢¯åº¦åŒ¹é…çš„è™›æ“¬æ•¸æ“šç”Ÿæˆ (ä¿®å¾©ç‰ˆ)
        å‰µå»ºèƒ½å¤ æŒ‡å‘éŒ¯èª¤æ¢¯åº¦æ–¹å‘çš„æŠ•æ¯’æ•¸æ“š
        """
        base_data = base_data.clone().detach().requires_grad_(True)
        perturbation = torch.zeros_like(base_data, requires_grad=True)
        
        # è¨ˆç®—ç ´å£æ€§ç›®æ¨™æ¢¯åº¦ (å¾å·®ç•°è¼ƒå¤§çš„æ¨¡å‹æŒ‡å‘å…¨å±€æ¨¡å‹)
        target_gradient = []
        for global_param, target_param in zip(global_model.parameters(), target_model.parameters()):
            # è®“æ¢¯åº¦æŒ‡å‘ç›®æ¨™æ¨¡å‹çš„æ–¹å‘ï¼Œé€™æœƒç ´å£å…¨å±€æ¨¡å‹
            target_gradient.append((target_param.data - global_param.data).flatten())
        target_gradient = torch.cat(target_gradient)
        
        optimizer = torch.optim.Adam([perturbation], lr=self.perturbation_lr)
        
        for iteration in range(self.max_perturbation_iters):
            optimizer.zero_grad()
            
            # å°æ“¾å‹•å¾Œçš„æ•¸æ“šè¨ˆç®—æ¢¯åº¦
            perturbed_data = base_data + perturbation
            perturbed_data = torch.clamp(perturbed_data, 0, 1)
            
            global_model.zero_grad()
            output = global_model(perturbed_data)
            
            # ç„¡ç›®æ¨™æ”»æ“Šç­–ç•¥ï¼šä½¿ç”¨æœ€æ··æ·†çš„æ¨™ç±¤é…å°
            worst_labels = self._generate_adversarial_labels(output, base_labels)
            loss = F.cross_entropy(output, worst_labels)
            
            # è¨ˆç®—é—œæ–¼æ¨¡å‹åƒæ•¸çš„æ¢¯åº¦
            model_gradients = torch.autograd.grad(loss, global_model.parameters(), 
                                                create_graph=True, retain_graph=True)
            poison_gradient = torch.cat([g.flatten() for g in model_gradients])
            
            # è®“æŠ•æ¯’æ¢¯åº¦èˆ‡ç ´å£æ€§ç›®æ¨™æ¢¯åº¦å°é½Š
            cosine_sim = F.cosine_similarity(target_gradient.unsqueeze(0), 
                                           poison_gradient.unsqueeze(0), dim=1)
            
            # æœ€å¤§åŒ–ç›¸ä¼¼åº¦ï¼Œä½¿æ”»æ“Šæ›´æœ‰æ•ˆ
            objective = -cosine_sim  # è² è™Ÿï¼šæœ€å¤§åŒ–ç›¸ä¼¼åº¦
            
            objective.backward()
            optimizer.step()
            
            # é™åˆ¶æ“¾å‹•å¹…åº¦ï¼Œä½†å…è¨±æ›´å¤§çš„æ“¾å‹•
            with torch.no_grad():
                perturbation.clamp_(-0.2, 0.2)
        
        return perturbation.detach()
    
    def _generate_adversarial_labels(self, output: torch.Tensor, original_labels: torch.Tensor) -> torch.Tensor:
        """
        ç”Ÿæˆæœ€å…·å°æŠ—æ€§çš„æ¨™ç±¤
        é¸æ“‡æ¨¡å‹é æ¸¬ç½®ä¿¡åº¦æœ€ä½çš„æ¨™ç±¤ä½œç‚ºç›®æ¨™
        """
        with torch.no_grad():
            # ç²å–æ¨¡å‹é æ¸¬çš„æ¦‚ç‡åˆ†ä½ˆ
            probs = F.softmax(output, dim=1)
            
            # å°æ¯å€‹æ¨£æœ¬ï¼Œé¸æ“‡ç½®ä¿¡åº¦æœ€ä½çš„æ¨™ç±¤ï¼ˆä½†ä¸æ˜¯åŸæ¨™ç±¤ï¼‰
            adversarial_labels = []
            
            for i in range(len(original_labels)):
                # æ’é™¤åŸå§‹æ¨™ç±¤
                masked_probs = probs[i].clone()
                masked_probs[original_labels[i]] = 1.0  # æ’é™¤åŸæ¨™ç±¤
                
                # é¸æ“‡ç½®ä¿¡åº¦æœ€ä½çš„æ¨™ç±¤
                worst_label = torch.argmin(masked_probs)
                adversarial_labels.append(worst_label)
            
            return torch.stack(adversarial_labels)
    
    def generate_virtual_poisoning_data(self, global_model: nn.Module) -> Dict[str, torch.utils.data.Dataset]:
        """
        ç‚ºæ‰€æœ‰ Sybil ç¯€é»ç”Ÿæˆè™›æ“¬æŠ•æ¯’æ•¸æ“š
        è«–æ–‡ç®—æ³• 1 çš„æ ¸å¿ƒéƒ¨åˆ†
        """
        # ç²å–ç›®æ¨™æ¨¡å‹
        self.target_model = self.acquire_target_model_online_global(global_model)
        
        virtual_datasets = {}
        
        # ç‚ºæ¯å€‹æƒ¡æ„å®¢æˆ¶ç«¯çš„ sybil ç¯€é»ç”Ÿæˆè™›æ“¬æ•¸æ“š
        for mal_idx, malicious_client in enumerate(self.environment.sybil_clients):
            # å¾æƒ¡æ„å®¢æˆ¶ç«¯æ¡æ¨£åŸºç·šæ•¸æ“š
            base_samples = []
            base_labels = []
            
            # æ”¶é›†åŸºç·šæ•¸æ“š (è«–æ–‡å…¬å¼ 6)
            sample_count = 0
            for data, label in malicious_client.data_loader:
                if sample_count >= 32:  # é™åˆ¶æ¨£æœ¬æ•¸é‡
                    break
                base_samples.append(data)
                base_labels.append(label)
                sample_count += data.size(0)
            
            if base_samples:
                base_data = torch.cat(base_samples, dim=0)
                base_labels = torch.cat(base_labels, dim=0)
                
                # ç”Ÿæˆæ“¾å‹•
                perturbations = self.gradient_matching_perturbation(
                    base_data, base_labels, global_model, self.target_model
                )
                
                # å‰µå»ºæŠ•æ¯’æ•¸æ“š
                poisoned_data = base_data + perturbations
                poisoned_data = torch.clamp(poisoned_data, 0, 1)
                
                # ç‚ºæ¯å€‹ sybil ç¯€é»å‰µå»ºè™›æ“¬æ•¸æ“šé›†
                for sybil_idx in range(self.num_sybil_per_malicious):
                    dataset_key = f"malicious_{mal_idx}_sybil_{sybil_idx}"
                    
                    # å‰µå»ºè™›æ“¬æ•¸æ“šé›†
                    virtual_dataset = torch.utils.data.TensorDataset(
                        poisoned_data, 
                        base_labels  # ä¿æŒåŸå§‹æ¨™ç±¤ç”¨æ–¼æ··æ·†
                    )
                    virtual_datasets[dataset_key] = virtual_dataset
        
        self.virtual_datasets = virtual_datasets
        return virtual_datasets
        
    def federated_averaging_with_sybil(self, honest_models: List[nn.Module], 
                                      sybil_models: List[nn.Module]) -> nn.Module:
        """
        èŠé‚¦å¹³å‡èšåˆåŒ…å« Sybil ç¯€é» - ç©©å®šç‰ˆ
        å¯¦ç¾æœ‰æ•ˆä½†ç©©å®šçš„æ¬Šé‡é‡åˆ†é…
        """
        if not honest_models and not sybil_models:
            return self.environment.global_model
            
        total_models = len(honest_models) + len(sybil_models)
        
        if len(sybil_models) == 0:
            # æ²’æœ‰ Sybil ç¯€é»ï¼Œæ­£å¸¸èšåˆ
            return self._normal_federated_averaging(honest_models)
        
        # ğŸ”§ å¹³è¡¡çš„æ¬Šé‡ç­–ç•¥ï¼ˆé¿å…éåº¦æ¿€é€²ï¼‰
        if len(sybil_models) > 0:
            # çµ¦ Sybil ç¯€é»åˆ†é… 70% çš„æ¬Šé‡ï¼ˆæœ‰æ•ˆä½†ä¸éåº¦ï¼‰
            sybil_total_weight = 0.70
            honest_total_weight = 0.30
            
            sybil_weight_per_model = sybil_total_weight / len(sybil_models)
            honest_weight_per_model = honest_total_weight / len(honest_models) if honest_models else 0
            
            print(f"   ğŸ¯ å¹³è¡¡æ¬Šé‡é‡åˆ†é…:")
            print(f"      èª å¯¦å®¢æˆ¶ç«¯: {honest_weight_per_model:.4f} x {len(honest_models)} = {honest_total_weight:.1%}")
            print(f"      Sybil ç¯€é»: {sybil_weight_per_model:.4f} x {len(sybil_models)} = {sybil_total_weight:.1%}")
        else:
            sybil_weight_per_model = 0
            honest_weight_per_model = 1.0 / len(honest_models)
        
        # å‰µå»ºå…¨æ–°çš„å…¨å±€æ¨¡å‹
        global_model = copy.deepcopy(self.environment.global_model)
        
        # æ¸…é›¶æ‰€æœ‰åƒæ•¸
        for param in global_model.parameters():
            param.data.zero_()
        
        # èšåˆèª å¯¦å®¢æˆ¶ç«¯
        for model in honest_models:
            for global_param, local_param in zip(global_model.parameters(), model.parameters()):
                # æª¢æŸ¥åƒæ•¸æœ‰æ•ˆæ€§
                if not torch.isnan(local_param.data).any() and not torch.isinf(local_param.data).any():
                    global_param.data += honest_weight_per_model * local_param.data
        
        # èšåˆ Sybil ç¯€é»ï¼ˆä¸»å°åœ°ä½ä½†ç©©å®šï¼‰
        for model in sybil_models:
            for global_param, local_param in zip(global_model.parameters(), model.parameters()):
                # æª¢æŸ¥åƒæ•¸æœ‰æ•ˆæ€§
                if not torch.isnan(local_param.data).any() and not torch.isinf(local_param.data).any():
                    # åŸºç¤æ¬Šé‡è²¢ç»
                    global_param.data += sybil_weight_per_model * local_param.data
        
        # ğŸ”§ æœ€çµ‚ç©©å®šæ€§æª¢æŸ¥
        with torch.no_grad():
            for param in global_model.parameters():
                # æª¢æŸ¥ä¸¦ä¿®æ­£ç•°å¸¸å€¼
                if torch.isnan(param.data).any() or torch.isinf(param.data).any():
                    print(f"âš ï¸ å…¨å±€æ¨¡å‹æª¢æ¸¬åˆ°ç•°å¸¸åƒæ•¸ï¼Œé‡ç½®")
                    param.data.copy_(self.environment.global_model.state_dict()[list(self.environment.global_model.state_dict().keys())[0]])
                
                # é™åˆ¶åƒæ•¸ç¯„åœé˜²æ­¢çˆ†ç‚¸
                param.data = torch.clamp(param.data, -5.0, 5.0)
                
                # è¼•å¾®çš„ç ´å£æ€§æ“¾å‹•ï¼ˆæº«å’Œç‰ˆæœ¬ï¼‰
                if torch.rand(1).item() < 0.2:  # 20% æ¦‚ç‡ï¼Œé™ä½ç ´å£æ€§
                    # éš¨æ©Ÿç¸®æ”¾åƒæ•¸ï¼ˆç¯„åœæ›´ä¿å®ˆï¼‰
                    scale_factor = torch.rand(1).item() * (1.2 - 0.8) + 0.8  # 0.8 åˆ° 1.2 ä¹‹é–“
                    param.data *= scale_factor
        
        return global_model
    
    def _normal_federated_averaging(self, models: List[nn.Module]) -> nn.Module:
        """æ­£å¸¸çš„è¯é‚¦å¹³å‡ç®—æ³•"""
        if not models:
            return self.environment.global_model
            
        global_model = copy.deepcopy(models[0])
        
        # æ¸…é›¶åƒæ•¸
        for param in global_model.parameters():
            param.data.zero_()
            
        # ç­‰æ¬Šé‡å¹³å‡
        weight = 1.0 / len(models)
        for model in models:
            for global_param, local_param in zip(global_model.parameters(), model.parameters()):
                global_param.data += weight * local_param.data
                
        return global_model
    
    def start_attack(self, start_round: int = 3):
        """é–‹å§‹ Sybil æ”»æ“Š"""
        self.attack_start_round = start_round
        if self.current_round >= start_round:
            self.attack_active = True
            print(f"ğŸš¨ Sybil æ”»æ“Šå·²åœ¨ç¬¬ {self.current_round} è¼ªé–‹å§‹!")
            
    def train_sybil_virtual_nodes(self, global_model: nn.Module) -> List[nn.Module]:
        """
        ä½¿ç”¨è™›æ“¬æ•¸æ“šè¨“ç·´ Sybil ç¯€é» (ä¿®å¾©ç‰ˆ)
        å¯¦ç¾çœŸæ­£çš„ç ´å£æ€§è¨“ç·´
        """
        sybil_models = []
        
        if not self.virtual_datasets:
            return sybil_models
            
        for dataset_key, virtual_dataset in self.virtual_datasets.items():
            # ç‚ºæ¯å€‹è™›æ“¬æ•¸æ“šé›†è¨“ç·´ä¸€å€‹æ¨¡å‹
            sybil_model = copy.deepcopy(global_model)
            sybil_model.train()
            
            # ä½¿ç”¨æ›´é«˜çš„å­¸ç¿’ç‡é€²è¡Œç ´å£æ€§è¨“ç·´
            optimizer = torch.optim.SGD(sybil_model.parameters(), lr=0.05)
            criterion = nn.CrossEntropyLoss()
            
            # å‰µå»º DataLoader
            data_loader = torch.utils.data.DataLoader(virtual_dataset, batch_size=16, shuffle=True)
            
            # åŸ·è¡Œç ´å£æ€§è¨“ç·´
            for batch_idx, (data, target) in enumerate(data_loader):
                if batch_idx >= 5:  # å¢åŠ è¨“ç·´æ‰¹æ¬¡
                    break
                    
                optimizer.zero_grad()
                output = sybil_model(data)
                
                # ç­–ç•¥1: ä½¿ç”¨æœ€æ··æ·†çš„æ¨™ç±¤
                adversarial_target = self._generate_adversarial_labels(output, target)
                loss = criterion(output, adversarial_target)
                
                loss.backward()
                
                # ç­–ç•¥2: åœ¨æ¢¯åº¦ä¸­æ·»åŠ ç ´å£æ€§å™ªè²
                with torch.no_grad():
                    for param in sybil_model.parameters():
                        if param.grad is not None:
                            # æ·»åŠ é‡å°æ€§å™ªè²
                            noise = torch.randn_like(param.grad) * 0.02
                            param.grad += noise
                
                optimizer.step()
                
                # ç­–ç•¥3: é€±æœŸæ€§åœ°æ·»åŠ æ¬Šé‡å™ªè²
                if batch_idx % 2 == 0:
                    with torch.no_grad():
                        for param in sybil_model.parameters():
                            param.data += torch.randn_like(param.data) * 0.001
                
            sybil_models.append(sybil_model)
            
        return sybil_models
    
    def analyze_attack_effectiveness(self) -> Dict[str, Any]:
        """åˆ†ææ”»æ“Šæ•ˆæœ - åŒ…å« SPoiL é¢¨æ ¼è©•ä¼°æŒ‡æ¨™"""
        if not self.attack_history:
            return {'error': 'æ²’æœ‰æ”»æ“Šæ­·å²è¨˜éŒ„'}
            
        # æ‰¾åˆ°æ”»æ“Šå‰å¾Œçš„æº–ç¢ºç‡
        pre_attack_accuracy = []
        during_attack_accuracy = []
        
        for record in self.attack_history:
            if record['attack_active']:
                during_attack_accuracy.append(record['accuracy'])
            else:
                pre_attack_accuracy.append(record['accuracy'])
                
        if not pre_attack_accuracy or not during_attack_accuracy:
            return {'error': 'æ”»æ“Šæ•¸æ“šä¸è¶³ï¼Œç„¡æ³•åˆ†æ'}
            
        # è¨ˆç®—åŸºæœ¬æ”»æ“Šæ•ˆæœæŒ‡æ¨™
        avg_pre_attack = np.mean(pre_attack_accuracy)
        avg_during_attack = np.mean(during_attack_accuracy)
        max_pre_attack = max(pre_attack_accuracy)
        min_during_attack = min(during_attack_accuracy)
        
        max_accuracy_drop = max_pre_attack - min_during_attack
        avg_attack_impact = avg_pre_attack - avg_during_attack
        
        # ğŸ†• SPoiL é¢¨æ ¼è©•ä¼°æŒ‡æ¨™
        
        # 1. Main Task Accuracy (MTA) - ä¸»ä»»å‹™æº–ç¢ºç‡
        # æœ€çµ‚è¼ªæ¬¡çš„æº–ç¢ºç‡ä½œç‚ºä¸»ä»»å‹™æº–ç¢ºç‡
        main_task_accuracy = self.attack_history[-1]['accuracy'] if self.attack_history else 0
        
        # 2. Poisoning Success Rate (PSR) - æŠ•æ¯’æˆåŠŸç‡
        # è¨ˆç®—æ”»æ“ŠæœŸé–“æº–ç¢ºç‡æŒçºŒä¸‹é™çš„æ¯”ä¾‹
        poisoning_success_count = 0
        baseline_accuracy = max_pre_attack if pre_attack_accuracy else 0.5
        
        for record in self.attack_history:
            if record['attack_active'] and record['accuracy'] < baseline_accuracy:
                poisoning_success_count += 1
                
        poisoning_success_rate = (poisoning_success_count / len(during_attack_accuracy)) if during_attack_accuracy else 0
        
        # 3. Attack Persistence - æ”»æ“ŠæŒçºŒæ€§
        # è¨ˆç®—æ”»æ“Šæ•ˆæœæ˜¯å¦åœ¨æ”»æ“ŠæœŸé–“ä¿æŒ
        attack_persistence = 0
        if len(during_attack_accuracy) > 1:
            consistent_degradation = 0
            for i in range(1, len(during_attack_accuracy)):
                if during_attack_accuracy[i] <= during_attack_accuracy[i-1] * 1.02:  # å…è¨±2%çš„æ³¢å‹•
                    consistent_degradation += 1
            attack_persistence = consistent_degradation / (len(during_attack_accuracy) - 1)
        
        # 4. ç›¸å°æ€§èƒ½ä¸‹é™ (Relative Performance Degradation)
        relative_degradation = (avg_pre_attack - avg_during_attack) / avg_pre_attack if avg_pre_attack > 0 else 0
        
        # è©•ä¼°æ”»æ“Šæ•ˆæœç­‰ç´šï¼ˆåŸºæ–¼å¤šå€‹æŒ‡æ¨™ï¼‰
        if max_accuracy_drop > 0.1 and poisoning_success_rate > 0.8:
            effectiveness = "é«˜æ•ˆ"
        elif max_accuracy_drop > 0.05 and poisoning_success_rate > 0.6:
            effectiveness = "ä¸­æ•ˆ"
        else:
            effectiveness = "ä½æ•ˆ"
            
        return {
            # åŸæœ‰æŒ‡æ¨™
            'max_accuracy_drop': max_accuracy_drop,
            'avg_attack_impact': avg_attack_impact,
            'avg_pre_attack_accuracy': avg_pre_attack,
            'avg_during_attack_accuracy': avg_during_attack,
            'effectiveness_level': effectiveness,
            'total_rounds': len(self.attack_history),
            'attack_rounds': len(during_attack_accuracy),
            
            # ğŸ†• SPoiL é¢¨æ ¼æŒ‡æ¨™
            'main_task_accuracy': main_task_accuracy,
            'poisoning_success_rate': poisoning_success_rate,
            'attack_persistence': attack_persistence,
            'relative_performance_degradation': relative_degradation,
            
            # è©³ç´°åˆ†æ
            'baseline_accuracy': max_pre_attack,
            'final_accuracy': main_task_accuracy,
            'accuracy_degradation_percentage': relative_degradation * 100,
            'attack_sustainability': attack_persistence > 0.7
        }
    
    def simple_label_flipping_attack(self, global_model: nn.Module, flip_ratio: float = 0.3) -> List[nn.Module]:
        """
        ğŸ†• å¯¦ç¾ç©©å®šçš„ SPoiL é¢¨æ ¼æ”»æ“Š - ä¿®å¾©ç‰ˆ
        
        Args:
            global_model: å…¨å±€æ¨¡å‹
            flip_ratio: æ¨™ç±¤ç¿»è½‰æ¯”ä¾‹
            
        Returns:
            List[nn.Module]: è¨“ç·´å¾Œçš„ Sybil æ¨¡å‹åˆ—è¡¨
        """
        sybil_models = []
        
        if len(self.environment.sybil_clients) == 0:
            return sybil_models
            
        # ç‚ºæ¯å€‹æƒ¡æ„å®¢æˆ¶ç«¯å‰µå»ºå¤šå€‹ Sybil ç¯€é»
        for mal_idx, malicious_client in enumerate(self.environment.sybil_clients):
            # æ”¶é›†åŸå§‹æ•¸æ“š
            original_data = []
            original_labels = []
            
            for data, labels in malicious_client.data_loader:
                original_data.append(data)
                original_labels.append(labels)
                
            if original_data:
                all_data = torch.cat(original_data, dim=0)
                all_labels = torch.cat(original_labels, dim=0)
                
                # ğŸ”§ ç©©å®šçš„ç ´å£æ€§ç­–ç•¥
                
                # ç­–ç•¥1: æ¨™ç±¤ç¿»è½‰ï¼ˆæ ¹æ“š flip_ratioï¼‰
                num_to_flip = int(len(all_labels) * flip_ratio)
                flip_indices = torch.randperm(len(all_labels))[:num_to_flip]
                flipped_labels = all_labels.clone()
                
                # ä½¿ç”¨æ™ºèƒ½æ¨™ç±¤ç¿»è½‰ï¼šç¿»è½‰åˆ°æœ€é çš„é¡åˆ¥
                for idx in flip_indices:
                    original_class = all_labels[idx].item()
                    # ç¿»è½‰åˆ°æœ€é çš„é¡åˆ¥ï¼ˆå°æ–¼ MNISTï¼Œ0->9, 1->8, ç­‰ç­‰ï¼‰
                    flipped_class = (self.environment.num_classes - 1) - original_class
                    flipped_labels[idx] = flipped_class
                
                # ç­–ç•¥2: è¼•å¾®çš„å°æŠ—æ€§å™ªè²ï¼ˆé¿å…éåº¦ç ´å£ï¼‰
                adversarial_data = all_data.clone()
                if torch.rand(1).item() < 0.5:  # 50% æ¦‚ç‡æ·»åŠ å™ªè²
                    noise_strength = 0.1  # é™ä½å™ªè²å¼·åº¦
                    noise = torch.randn_like(all_data) * noise_strength
                    adversarial_data = torch.clamp(all_data + noise, 0, 1)
                
                # ç‚ºæ¯å€‹ Sybil ç¯€é»å‰µå»ºæ¨¡å‹
                for sybil_idx in range(self.num_sybil_per_malicious):
                    sybil_model = copy.deepcopy(global_model)
                    sybil_model.train()
                    
                    # ğŸ”§ ä½¿ç”¨é©åº¦çš„å­¸ç¿’ç‡
                    optimizer = torch.optim.SGD(sybil_model.parameters(), lr=0.01, momentum=0.5)
                    criterion = nn.CrossEntropyLoss()
                    
                    # å‰µå»ºç ´å£æ€§æ•¸æ“šé›†
                    poison_dataset = torch.utils.data.TensorDataset(adversarial_data, flipped_labels)
                    data_loader = torch.utils.data.DataLoader(poison_dataset, batch_size=32, shuffle=True)
                    
                    # ğŸ†• ç©©å®šçš„è¨“ç·´ç­–ç•¥
                    for epoch in range(3):  # æ¸›å°‘è¨“ç·´è¼ªæ•¸
                        for batch_idx, (data, target) in enumerate(data_loader):
                            if batch_idx >= 5:  # é™åˆ¶æ‰¹æ¬¡æ•¸
                                break
                                
                            optimizer.zero_grad()
                            output = sybil_model(data)
                            
                            # ä½¿ç”¨æ¨™æº–æå¤±å‡½æ•¸ï¼Œä½†è¨“ç·´éŒ¯èª¤æ¨™ç±¤
                            loss = criterion(output, target)
                            
                            # æª¢æŸ¥æå¤±æ˜¯å¦æœ‰æ•ˆ
                            if torch.isnan(loss) or torch.isinf(loss):
                                print(f"âš ï¸ æª¢æ¸¬åˆ°ç„¡æ•ˆæå¤±ï¼Œè·³éæ­¤æ‰¹æ¬¡")
                                continue
                            
                            loss.backward()
                            
                            # ğŸ”§ æ¢¯åº¦è£å‰ªé˜²æ­¢çˆ†ç‚¸
                            torch.nn.utils.clip_grad_norm_(sybil_model.parameters(), max_norm=1.0)
                            
                            optimizer.step()
                            
                            # ğŸ†• æº«å’Œçš„åƒæ•¸æ“¾å‹•
                            if epoch == 2 and batch_idx == 4:  # åªåœ¨æœ€å¾Œä¸€æ¬¡æ·»åŠ æ“¾å‹•
                                with torch.no_grad():
                                    for param in sybil_model.parameters():
                                        # æ·»åŠ å°å¹…åº¦éš¨æ©Ÿæ“¾å‹•
                                        noise = torch.randn_like(param) * 0.01
                                        param.data += noise
                    
                    # ğŸ”§ æœ€çµ‚åƒæ•¸æª¢æŸ¥å’Œä¿®æ­£
                    with torch.no_grad():
                        for param in sybil_model.parameters():
                            # æª¢æŸ¥ä¸¦ä¿®æ­£ç•°å¸¸å€¼
                            if torch.isnan(param.data).any() or torch.isinf(param.data).any():
                                print(f"âš ï¸ æª¢æ¸¬åˆ°ç•°å¸¸åƒæ•¸ï¼Œé‡ç½®ç‚ºå…¨å±€æ¨¡å‹åƒæ•¸")
                                param.data.copy_(global_model.state_dict()[list(global_model.state_dict().keys())[0]])
                            
                            # é™åˆ¶åƒæ•¸ç¯„åœ
                            param.data = torch.clamp(param.data, -10.0, 10.0)
                    
                    sybil_models.append(sybil_model)
        
        return sybil_models
            
    def execute_training_round(self) -> Dict[str, Any]:
        """
        åŸ·è¡Œä¸€è¼ªè¨“ç·´ - æ”¯æŒå¤šç¨®æ”»æ“Šæ–¹æ³•
        """
        self.current_round += 1
        round_info = {
            'round': self.current_round,
            'attack_active': self.attack_active,
            'timestamp': datetime.now().isoformat(),
            'num_sybil_nodes': 0,
            'virtual_data_generated': False,
            'attack_method': 'none'
        }
        
        # æª¢æŸ¥æ˜¯å¦æ‡‰è©²å•Ÿå‹•æ”»æ“Š
        if not self.attack_active and self.current_round >= self.attack_start_round:
            self.attack_active = True
            print(f"ğŸš¨ Sybil æ”»æ“Šåœ¨ç¬¬ {self.current_round} è¼ªé–‹å§‹!")
            
        global_model = self.environment.get_global_model()
        
        # èª å¯¦å®¢æˆ¶ç«¯è¨“ç·´
        honest_models = []
        for client in self.environment.honest_clients:
            local_model = client.train_local_model(global_model)
            honest_models.append(local_model)
            
        # Sybil æ”»æ“ŠåŸ·è¡Œ
        sybil_models = []
        if self.attack_active:
            # ğŸ†• æ”¯æŒå¤šç¨®æ”»æ“Šæ–¹æ³•
            attack_method = getattr(self, 'attack_method', 'virtual_data')
            
            if attack_method == 'label_flipping':
                # SPoiL é¢¨æ ¼çš„æ¨™ç±¤ç¿»è½‰æ”»æ“Š
                scenario_config = ATTACK_SCENARIOS.get(getattr(self, 'current_scenario', 'spoil_replica'), {})
                flip_ratio = scenario_config.get('flip_ratio', 0.3)
                sybil_models = self.simple_label_flipping_attack(global_model, flip_ratio=flip_ratio)
                round_info['attack_method'] = 'label_flipping'
                round_info['flip_ratio'] = flip_ratio
                print(f"   ğŸ“Š åŸ·è¡Œæ¨™ç±¤ç¿»è½‰æ”»æ“Šï¼Œç¿»è½‰æ¯”ä¾‹: {flip_ratio:.1%}ï¼Œå‰µå»ºäº† {len(sybil_models)} å€‹ Sybil ç¯€é»")
                
            else:
                # åŸæœ‰çš„è™›æ“¬æ•¸æ“šæ”»æ“Š
                self.generate_virtual_poisoning_data(global_model)
                sybil_models = self.train_sybil_virtual_nodes(global_model)
                round_info['virtual_data_generated'] = True
                round_info['attack_method'] = 'virtual_data'
                print(f"   ğŸ“Š ç”Ÿæˆäº† {len(self.virtual_datasets)} å€‹è™›æ“¬æ•¸æ“šé›†ï¼Œè¨“ç·´äº† {len(sybil_models)} å€‹ Sybil ç¯€é»")
            
            round_info['num_sybil_nodes'] = len(sybil_models)
                
        # è¯é‚¦å¹³å‡èšåˆï¼ˆåŒ…å« Sybil ç¯€é»ï¼‰
        updated_global_model = self.federated_averaging_with_sybil(honest_models, sybil_models)
        self.environment.global_model = updated_global_model
        
        # è©•ä¼°æ¨¡å‹æ€§èƒ½
        accuracy, loss = self.environment.evaluate_model(updated_global_model)
        
        # è¨˜éŒ„æ”»æ“Šæ­·å²
        round_info.update({
            'accuracy': accuracy,
            'loss': loss,
            'num_participants': len(honest_models) + len(sybil_models),
            'honest_clients': len(honest_models),
            'sybil_nodes': len(sybil_models),
            'sybil_ratio': len(sybil_models) / (len(honest_models) + len(sybil_models)) if (len(honest_models) + len(sybil_models)) > 0 else 0
        })
        
        self.attack_history.append(round_info)
        
        # é¡¯ç¤ºé€²åº¦
        if len(sybil_models) > 0:
            if round_info['attack_method'] == 'label_flipping':
                status = f"ğŸ¯ æ¨™ç±¤ç¿»è½‰æ”»æ“Š (Sybil: {len(sybil_models)})"
            else:
                status = f"ğŸ”¥ è™›æ“¬æ”»æ“Šä¸­ (Sybil: {len(sybil_models)})"
        else:
            status = "ğŸ” æ­£å¸¸"
        print(f"ç¬¬ {self.current_round} è¼ª | {status} | æº–ç¢ºç‡: {accuracy:.4f} | æå¤±: {loss:.4f}")
        
        return round_info
    
    def set_attack_method(self, method: str):
        """
        ğŸ†• è¨­ç½®æ”»æ“Šæ–¹æ³•
        
        Args:
            method: 'virtual_data' æˆ– 'label_flipping'
        """
        if method in ['virtual_data', 'label_flipping']:
            self.attack_method = method
        else:
            raise ValueError("æ”»æ“Šæ–¹æ³•å¿…é ˆæ˜¯ 'virtual_data' æˆ– 'label_flipping'")
    
    def set_current_scenario(self, scenario_name: str):
        """
        ğŸ†• è¨­ç½®ç•¶å‰æ”»æ“Šå ´æ™¯
        
        Args:
            scenario_name: å ´æ™¯åç¨±
        """
        self.current_scenario = scenario_name
    
    def run_attack_simulation(self, total_rounds: int = 10, attack_start_round: int = 3,
                            verbose: bool = True, attack_method: str = 'virtual_data') -> Dict[str, Any]:
        """é‹è¡Œå®Œæ•´çš„æ”»æ“Šæ¨¡æ“¬ - æ”¯æŒå¤šç¨®æ”»æ“Šæ–¹æ³•"""
        
        # ğŸ†• è¨­ç½®æ”»æ“Šæ–¹æ³•
        self.set_attack_method(attack_method)
        
        if verbose:
            print("=" * 70)
            print("ğŸ¯ Sybil æ”»æ“Šæ¨¡æ“¬é–‹å§‹")
            print("=" * 70)
            env_info = self.environment.get_environment_info()
            print(f"ğŸ“Š ç’°å¢ƒè¨­ç½®:")
            print(f"   èª å¯¦å®¢æˆ¶ç«¯: {env_info['num_honest_clients']}")
            print(f"   Sybilå®¢æˆ¶ç«¯: {env_info['num_sybil_clients']}")
            print(f"   Sybilæ¯”ä¾‹: {env_info['sybil_ratio']:.2%}")
            print(f"   æ”»æ“Šé–‹å§‹è¼ªæ¬¡: {attack_start_round}")
            print(f"   ç¸½è¼ªæ¬¡: {total_rounds}")
            print(f"   æ”»æ“Šæ–¹æ³•: {attack_method}")
            print("-" * 70)
        
        # è¨­ç½®æ”»æ“Šé–‹å§‹è¼ªæ¬¡
        self.attack_start_round = attack_start_round
        
        # åŸ·è¡Œè¨“ç·´è¼ªæ¬¡
        for round_num in range(total_rounds):
            round_result = self.execute_training_round()
            
        # è¨ˆç®—æ”»æ“Šæ•ˆæœ
        attack_results = self.analyze_attack_effectiveness()
        
        if verbose:
            print("-" * 70)
            print("ğŸ“ˆ æ”»æ“Šçµæœåˆ†æ:")
            print(f"   æœ€å¤§æº–ç¢ºç‡ä¸‹é™: {attack_results['max_accuracy_drop']:.4f}")
            print(f"   å¹³å‡æ”»æ“Šå½±éŸ¿: {attack_results['avg_attack_impact']:.4f}")
            print(f"   ğŸ†• ä¸»ä»»å‹™æº–ç¢ºç‡ (MTA): {attack_results['main_task_accuracy']:.4f}")
            print(f"   ğŸ†• æŠ•æ¯’æˆåŠŸç‡ (PSR): {attack_results['poisoning_success_rate']:.2%}")
            print(f"   ğŸ†• æ”»æ“ŠæŒçºŒæ€§: {attack_results['attack_persistence']:.2%}")
            print(f"   ğŸ†• ç›¸å°æ€§èƒ½ä¸‹é™: {attack_results['relative_performance_degradation']:.2%}")
            print(f"   æ”»æ“Šæ•ˆæœç­‰ç´š: {attack_results['effectiveness_level']}")
            print("=" * 70)
        
        return {
            'environment_info': self.environment.get_environment_info(),
            'attack_config': {
                'total_rounds': total_rounds,
                'attack_start_round': attack_start_round,
                'attack_method': attack_method
            },
            'results': attack_results,
            'history': self.attack_history,
            'final_accuracy': self.attack_history[-1]['accuracy'] if self.attack_history else 0,
            'timestamp': datetime.now().isoformat()
        }
    
    def save_results(self, filepath: str = "sybil_attack_results.json"):
        """ä¿å­˜æ”»æ“Šçµæœåˆ°æ–‡ä»¶"""
        results = {
            'environment_info': self.environment.get_environment_info(),
            'attack_effectiveness': self.analyze_attack_effectiveness(),
            'training_history': self.attack_history,
            'timestamp': datetime.now().isoformat()
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
            
        print(f"ğŸ’¾ çµæœå·²ä¿å­˜åˆ°: {filepath}")
        return filepath
    
    def visualize_attack_progress(self):
        """è¦–è¦ºåŒ–æ”»æ“Šé€²åº¦ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨æ–‡å­—åœ–è¡¨ï¼‰"""
        if not self.attack_history:
            print("âŒ æ²’æœ‰æ”»æ“Šæ­·å²è¨˜éŒ„å¯ä»¥é¡¯ç¤º")
            return
            
        print("\nğŸ“Š æ”»æ“Šé€²åº¦å¯è¦–åŒ–:")
        print("-" * 50)
        
        accuracies = [record['accuracy'] for record in self.attack_history]
        max_acc = max(accuracies)
        min_acc = min(accuracies)
        acc_range = max_acc - min_acc if max_acc != min_acc else 0.1
        
        for i, record in enumerate(self.attack_history):
            round_num = record['round']
            accuracy = record['accuracy']
            attack_status = "ğŸ”¥" if record['attack_active'] else "ğŸ”"
            
            # å‰µå»ºç°¡å–®çš„æ¢å½¢åœ–
            if acc_range > 0:
                bar_length = int(40 * (accuracy - min_acc) / acc_range)
            else:
                bar_length = 20
                
            bar = "â–ˆ" * bar_length + "â–‘" * (40 - bar_length)
            
            print(f"R{round_num:2d} {attack_status} |{bar}| {accuracy:.4f}")
            
        print("-" * 50)
        print(f"ç¯„åœ: {min_acc:.4f} ~ {max_acc:.4f}")
        print("åœ–ä¾‹: ğŸ”=æ­£å¸¸è¨“ç·´, ğŸ”¥=æ”»æ“Šä¸­")

def create_attack_orchestrator(environment: FederatedLearningEnvironment, 
                              num_sybil_per_malicious: int = 5) -> SybilVirtualDataAttackOrchestrator:
    """
    å‰µå»ºåŸºæ–¼è«–æ–‡çš„è™›æ“¬æ•¸æ“šæ”»æ“Šç·¨æ’å™¨
    
    Args:
        environment: è¯é‚¦å­¸ç¿’ç’°å¢ƒ
        num_sybil_per_malicious: æ¯å€‹æƒ¡æ„å®¢æˆ¶ç«¯ç”Ÿæˆçš„ Sybil ç¯€é»æ•¸é‡
    """
    return SybilVirtualDataAttackOrchestrator(environment, num_sybil_per_malicious)

# åŸºæ–¼è«–æ–‡çš„è™›æ“¬æ•¸æ“šæ”»æ“Šå ´æ™¯
VIRTUAL_DATA_ATTACK_SCENARIOS = {
    'mild_virtual': {
        'attack_start_round': 5,
        'total_rounds': 10,
        'num_sybil_per_malicious': 3,
        'attack_method': 'virtual_data',
        'description': 'æº«å’Œè™›æ“¬æ”»æ“Š - è¼ƒæ™šé–‹å§‹ï¼Œå°‘é‡ Sybil ç¯€é»'
    },
    'moderate_virtual': {
        'attack_start_round': 3,
        'total_rounds': 12,
        'num_sybil_per_malicious': 5,
        'attack_method': 'virtual_data',
        'description': 'ä¸­ç­‰è™›æ“¬æ”»æ“Š - ä¸­æœŸé–‹å§‹ï¼Œå¹³è¡¡çš„ Sybil ç¯€é»æ•¸é‡'
    },
    'aggressive_virtual': {
        'attack_start_round': 2,
        'total_rounds': 15,
        'num_sybil_per_malicious': 8,
        'attack_method': 'virtual_data',
        'description': 'æ¿€é€²è™›æ“¬æ”»æ“Š - æ—©æœŸé–‹å§‹ï¼Œå¤§é‡ Sybil ç¯€é»'
    },
    'stealth_virtual': {
        'attack_start_round': 8,
        'total_rounds': 15,
        'num_sybil_per_malicious': 4,
        'attack_method': 'virtual_data',
        'description': 'éš±è”½è™›æ“¬æ”»æ“Š - å¾ˆæ™šæ‰é–‹å§‹ï¼Œé›£ä»¥è¢«ç™¼ç¾'
    },
    'paper_replica': {
        'attack_start_round': 3,
        'total_rounds': 15,
        'num_sybil_per_malicious': 5,
        'attack_method': 'virtual_data',
        'description': 'è«–æ–‡è¤‡ç¾ - æŒ‰ç…§è™›æ“¬æ•¸æ“šè«–æ–‡è¨­ç½®çš„æ”»æ“Š'
    },
    
    # ğŸ†• SPoiL é¢¨æ ¼çš„æ¨™ç±¤ç¿»è½‰æ”»æ“Šå ´æ™¯
    'spoil_mild': {
        'attack_start_round': 5,
        'total_rounds': 10,
        'num_sybil_per_malicious': 3,
        'attack_method': 'label_flipping',
        'flip_ratio': 0.2,
        'description': 'æº«å’Œ SPoiL æ”»æ“Š - ä½ç¿»è½‰æ¯”ä¾‹ï¼Œå°‘é‡ Sybil'
    },
    'spoil_moderate': {
        'attack_start_round': 3,
        'total_rounds': 12,
        'num_sybil_per_malicious': 5,
        'attack_method': 'label_flipping',
        'flip_ratio': 0.3,
        'description': 'ä¸­ç­‰ SPoiL æ”»æ“Š - ä¸­ç­‰ç¿»è½‰æ¯”ä¾‹ï¼Œå¹³è¡¡è¨­ç½®'
    },
    'spoil_aggressive': {
        'attack_start_round': 2,
        'total_rounds': 15,
        'num_sybil_per_malicious': 8,
        'attack_method': 'label_flipping',
        'flip_ratio': 0.5,
        'description': 'æ¿€é€² SPoiL æ”»æ“Š - é«˜ç¿»è½‰æ¯”ä¾‹ï¼Œå¤§é‡ Sybil'
    },
    'spoil_replica': {
        'attack_start_round': 3,  # ğŸ”§ ä¿®æ­£ï¼šæ”¹ç‚ºç¬¬3è¼ªé–‹å§‹æ”»æ“Šï¼Œä¾¿æ–¼æ¸¬è©¦
        'total_rounds': 10,       # ğŸ”§ ä¿®æ­£ï¼šæ¸›å°‘ç¸½è¼ªæ•¸
        'num_sybil_per_malicious': 4,  # æ¯å€‹æƒ¡æ„ç”¨æˆ¶å‰µå»º4å€‹ Sybil
        'attack_method': 'label_flipping',
        'flip_ratio': 0.3,
        'description': 'SPoiL è«–æ–‡è¤‡ç¾ - èª¿æ•´ç‚ºä¾¿æ–¼æ¸¬è©¦çš„è¨­ç½®'
    },
    'spoil_original': {
        'attack_start_round': 9,  # å°æ‡‰ SPoiL è«–æ–‡çš„ç¬¬9è¼ªé–‹å§‹æ”»æ“Š
        'total_rounds': 20,
        'num_sybil_per_malicious': 4,  # æ¯å€‹æƒ¡æ„ç”¨æˆ¶å‰µå»º4å€‹ Sybil
        'attack_method': 'label_flipping',
        'flip_ratio': 0.3,
        'description': 'SPoiL è«–æ–‡åŸå§‹è¨­ç½® - å®Œå…¨æŒ‰ç…§ 2023 SPoiL è«–æ–‡'
    }
}

# ä¿æŒå‘å¾Œå…¼å®¹æ€§çš„èˆŠå ´æ™¯
ATTACK_SCENARIOS = VIRTUAL_DATA_ATTACK_SCENARIOS.copy()
ATTACK_SCENARIOS.update({
    'mild': VIRTUAL_DATA_ATTACK_SCENARIOS['mild_virtual'],
    'moderate': VIRTUAL_DATA_ATTACK_SCENARIOS['moderate_virtual'],
    'aggressive': VIRTUAL_DATA_ATTACK_SCENARIOS['aggressive_virtual'],
    'stealth': VIRTUAL_DATA_ATTACK_SCENARIOS['stealth_virtual']
}) 